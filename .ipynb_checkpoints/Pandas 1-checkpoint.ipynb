{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Seven 7 \n",
    "# Programming with Python  MOD007891"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation with Pandas\n",
    "\n",
    "\n",
    "# Outline:\n",
    "\n",
    " - Installing and Using Pandas\n",
    " - Pandas Objects\n",
    " - Pandas Series Object\n",
    " - Pandas DataFrame Object\n",
    " - Pandas Index Object\n",
    " - Data Indexing and Selection\n",
    " - Handling Missing Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Datasets\n",
    "\n",
    "- state-population.csv\n",
    "- state-areas.csv\n",
    "- state-abbrevs.csv\n",
    "- births.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous sessions, we dove into detail on **NumPy** and its ``ndarray`` object, which provides **efficient storage** and **manipulation** of dense typed arrays in Python.\n",
    "\n",
    "\n",
    "Here we'll build on this knowledge by looking in detail at the **data structures** provided by the Pandas library.\n",
    "\n",
    "\n",
    "Pandas is a newer package **built on top of NumPy**, and provides an efficient implementation of a ``DataFrame``.\n",
    "\n",
    "\n",
    "``DataFrame``s are essentially **multidimensional arrays** with **attached row and column labels**, and often with **heterogeneous types** and/or **missing data.**\n",
    "\n",
    "\n",
    "As well as offering a **convenient storage** interface for labeled data, Pandas implements a number of powerful data operations familiar to users of both database frameworks and spreadsheet programs.\n",
    "\n",
    "\n",
    "\n",
    "**Numpy** limitations become clear when we need more **flexibility** (e.g., **attaching labels** to data, working with **missing data**, etc.).\n",
    "\n",
    "\n",
    "Pandas, and in particular **its ``Series`` and ``DataFrame`` objects**, builds on the **NumPy array** structure and provides efficient access to these sorts of \"data munging\" tasks that occupy much of a data scientist's time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Using Pandas\n",
    "\n",
    "Installation of Pandas on your system **requires NumPy** to be installed.\n",
    "\n",
    "\n",
    "\n",
    "Details on this installation can be found in the [Pandas documentation](http://pandas.pydata.org/).\n",
    "\n",
    "\n",
    "Once Pandas is installed, you can import it and check the version:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "pandas.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we generally import NumPy under the alias ``np``, we will import Pandas under the alias ``pd``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This import convention will be used throughout the remainder of this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder about Built-In Documentation\n",
    "\n",
    "As you read through this chapter, don't forget that IPython gives you the ability to quickly explore the contents of a package (by using the tab-completion feature) as well as the documentation of various functions (using the ``?`` character). (Refer back to [Help and Documentation in IPython](01.01-Help-And-Documentation.ipynb) if you need a refresher on this.)\n",
    "\n",
    "For example, to display all the contents of the pandas namespace, you can type\n",
    "\n",
    "```ipython\n",
    "In [3]: pd.<TAB>\n",
    "```\n",
    "\n",
    "And to display Pandas's built-in documentation, you can use this:\n",
    "\n",
    "```ipython\n",
    "In [4]: pd?\n",
    "```\n",
    "\n",
    "More detailed documentation, along with tutorials and other resources, can be found at http://pandas.pydata.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Pandas Objects\n",
    "\n",
    "\n",
    "At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the **rows and columns are identified with labels rather than simple integer indices.**\n",
    "\n",
    "\n",
    "Pandas provides a host of useful tools, methods, and functionality on top of the basic **data structures.**\n",
    "\n",
    "\n",
    "Thus, before we go any further, let's introduce these **three fundamental Pandas data structures**:\n",
    "\n",
    "- ``Series`` \n",
    "- ``DataFrame``\n",
    "- ``Index``\n",
    "\n",
    "We will start our code sessions with the standard NumPy and Pandas imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pandas Series Object\n",
    "\n",
    "A Pandas ``Series`` is a **one-dimensional array of indexed data.**\n",
    "It can be created from a list or array as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75, 1.0])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the output, the ``Series`` wraps both a **sequence of values** and a **sequence of indices**, which we can access with the ``values`` and ``index`` attributes.\n",
    "\n",
    "\n",
    "The ``values`` are simply a **NumPy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``index`` is an **array-like** object of type ``pd.Index``, which we'll discuss in more detail momentarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with a NumPy array, data can be **accessed** by the **associated index** via the familiar Python square-bracket notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see, though, the Pandas ``Series`` is much more general and flexible than the one-dimensional NumPy array that it emulates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Series`` as generalized NumPy array\n",
    "\n",
    "\n",
    "From what we've seen so far, it may look like the ``Series`` object is basically **interchangeable** with a **one-dimensional NumPy array.**\n",
    "\n",
    "\n",
    "The essential difference is the **presence of the index:** while the Numpy Array has an ***implicitly defined*** integer index used to access the values, the Pandas ``Series`` has an ***explicitly defined*** index associated with the values.\n",
    "\n",
    "This explicit index definition gives the ``Series`` object **additional capabilities.**9\n",
    "\n",
    "For example, the index **need not be an integer,** but can consist of values of any desired type.\n",
    "For example, if we wish, we can use strings as an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the index in Pandas Series need not be an integer\n",
    "data = pd.Series([0.25, 0.5, 0.75, 1.0], index=['a', 'b', 'c', 'd'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the item access works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use **non-contiguous** or **non-sequential** indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75, 1.0],\n",
    "                 index=[2, 5, 3, 7])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series as specialized dictionary\n",
    "\n",
    "You can think of a Pandas ``Series`` a bit like a specialization of a **Python dictionary.**\n",
    "\n",
    "\n",
    "A dictionary is a structure that maps **arbitrary keys** to a set of **arbitrary values**, and a Pandas ``Series`` is a structure which **maps typed keys** to a set of **typed values.**\n",
    "\n",
    "But because Pandas ``Series`` backed by **Numpy array,** it makes it much **more efficient** than **Python dictionaries** for certain operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a Pandas Series object directly from a Python dictionary\n",
    "\n",
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 26448193,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "\n",
    "population = pd.Series(population_dict)\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population['California']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike a dictionary, though, the ``Series`` also supports array-style operations such as **slicing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population['California':'New York']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Series objects\n",
    "\n",
    "We've already seen a few ways of constructing a Pandas ``Series`` from scratch; all of them are some version of the following:\n",
    "\n",
    "```python\n",
    ">>> pd.Series(data, index=index)\n",
    "```\n",
    "\n",
    "where ``index`` is an **optional** argument, and ``data`` can be one of many entities.\n",
    "\n",
    "For example, ``data`` can be a **list** or **NumPy array**, in which case ``index`` defaults to an integer sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where index is an optional argument\n",
    "pd.Series([2, 4, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit indexing \n",
    "pd.Series([0.25, 0.5, 0.75, 1.0], index=['a', 'b', 'c', 'd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``data`` can be a scalar, which is **repeated to fill** the specified index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(5, index=[100, 200, 300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``data`` can be a **dictionary**, in which ``index`` defaults to the sorted dictionary keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series({2:'a', 1:'b', 3:'c'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each case, the index can be **explicitly** set if a different result is preferred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series({2:'a', 1:'b', 3:'c'}, index=[3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in this case, the ``Series`` is populated only with the explicitly identified keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pandas DataFrame Object\n",
    "\n",
    "The next fundamental structure in Pandas is the ``DataFrame``.\n",
    "\n",
    "\n",
    "The ``DataFrame`` can be thought of either as a **generalization of a NumPy array**, or as a specialization of a **Python dictionary.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame as a generalized NumPy array\n",
    "\n",
    "\n",
    "If a ``Series`` is an analog of a **one-dimensional array** with flexible indices, a ``DataFrame`` is an analog of a **two-dimensional array** with both flexible row indices and flexible column names.\n",
    "\n",
    "\n",
    "You can think of a ``DataFrame`` as a sequence of aligned ``Series`` objects.\n",
    "\n",
    "\n",
    "Here, by ***\"aligned\"*** we mean that they **share the same index.**\n",
    "\n",
    "\n",
    "- **Let's demonstrate this:**\n",
    "\n",
    "\n",
    "\n",
    "let's first construct a new ``Series`` listing the area of each of the five states discussed in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\n",
    "             'Florida': 170312, 'Illinois': 149995}\n",
    "area = pd.Series(area_dict)\n",
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 26448193,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "\n",
    "population = pd.Series(population_dict)\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have `` area `` along with the ``population`` Series, we can use a **dictionary** to construct a single two-dimensional object containing this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.DataFrame({'Population': population,'Area': area})\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the ``Series`` object, the ``DataFrame`` has an ``index`` attribute that gives access to the index labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the ``DataFrame`` has a ``columns`` attribute, which is an ``Index`` object holding the column labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame as specialized dictionary\n",
    "\n",
    "Similarly, we can also think of a ``DataFrame`` as a **specialization of a dictionary.**\n",
    "\n",
    "\n",
    "Where a dictionary **maps a key to a value**, a ``DataFrame`` maps a **column name** to a ``Series`` of **column data.**\n",
    "\n",
    "\n",
    "For example, asking for the ``'area'`` attribute returns the ``Series`` object containing the areas we saw earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the potential point of confusion here: in a two-dimesnional NumPy array, ``data[0]`` will return the first *row*. \n",
    "\n",
    "For a ``DataFrame``, ``data['col0']`` will return the first *column*.\n",
    "\n",
    "\n",
    "Because of this, it is probably better to think about ``DataFrame``s as generalized dictionaries rather than generalized arrays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing DataFrame objects\n",
    "\n",
    "A Pandas ``DataFrame`` can be constructed in a variety of ways.\n",
    "Here we'll give several examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a single Series object\n",
    "\n",
    "A ``DataFrame`` is a collection of ``Series`` objects, and a single-column ``DataFrame`` can be constructed from a single ``Series``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a single Pandas Series to a Pandas DataFrame\n",
    "x = pd.DataFrame(population, columns=['population'])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a list of dicts\n",
    "\n",
    "Any list of dictionaries can be made into a ``DataFrame``.\n",
    "We'll use a simple list comprehension to create some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'a': i, 'b': 2 * i} for i in range(3)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if some keys in the dictionary are missing, Pandas will fill them in with ``NaN`` (i.e., \"not a number\") values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row missing c, Second row missing a\n",
    "pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a dictionary of Series objects\n",
    "\n",
    "As we saw before, a ``DataFrame`` can be constructed from a dictionary of ``Series`` objects as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe made from 2 Series using \"\"dictionary of Series\"\" syntax\n",
    "pd.DataFrame({'population': population,'area': area})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a two-dimensional NumPy array\n",
    "\n",
    "Given a two-dimensional array of data, we can create a ``DataFrame`` with any specified column and index names.\n",
    "If omitted, an integer index will be used for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.random.rand(3, 2), columns=['foo', 'bar'], index=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pandas Index Object\n",
    "\n",
    "We have seen here that both the ``Series`` and ``DataFrame`` objects contain an **explicit *index*** that lets you reference and modify data.\n",
    "\n",
    "\n",
    "This ``Index`` object is an interesting structure in itself, and it can be thought of either as:\n",
    "\n",
    "-  *Immutable Array* \n",
    "-  *Ordered Set* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = pd.Index([2, 3, 5, 7, 11])\n",
    "ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index as immutable array\n",
    "\n",
    "The ``Index`` in many ways operates like an array.\n",
    "For example, we can use standard Python indexing notation to retrieve values or slices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Index`` objects also have many of the attributes familiar from NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ind.size, ind.shape, ind.ndim, ind.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difference between ``Index`` objects and NumPy arrays is that **indices are immutable** and they **cannot be modified** via the normal means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **immutability** makes it **safer** to share indices between multiple ``DataFrame``s and arrays, without the potential for side effects from inadvertent index modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index as ordered set\n",
    "\n",
    "Pandas objects are designed to facilitate operations such as **joins** across datasets.\n",
    "\n",
    "\n",
    "The ``Index`` object follows many of the conventions used by Python's built-in ``set`` data structure, so that **unions, intersections, differences,** and other combinations can be computed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indA = pd.Index([1, 3, 5, 7, 9])\n",
    "indB = pd.Index([2, 3, 5, 7, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indA & indB  # intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indA | indB  # union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indA ^ indB  # symmetric difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations may also be accessed via object methods, for example ``indA.intersection(indB)``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Indexing and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we looked in detail at methods and tools to access, set, and modify values in NumPy arrays.\n",
    "\n",
    "\n",
    "These included:\n",
    "\n",
    "- indexing (e.g., ``arr[2, 1]``)\n",
    "- slicing (e.g., ``arr[:, 1:5]``)\n",
    "- masking (e.g., ``arr[arr > 0]``), \n",
    "- fancy indexing (e.g., ``arr[0, [1, 5]]``)\n",
    "- combinations thereof (e.g., ``arr[:, [1, 5]]``).\n",
    "\n",
    "\n",
    "Here we'll look at **similar means** of **accessing** and **modifying** values in Pandas ``Series`` and ``DataFrame`` objects.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection in Series\n",
    "\n",
    "As we saw in the previous section, a ``Series`` object acts in many ways like:\n",
    "\n",
    "\n",
    "- A standard **Python dictionary**\n",
    "- A one-dimensional **NumPy array**\n",
    "\n",
    "\n",
    "\n",
    "If we keep these two overlapping analogies in mind, it will help us to understand the patterns of data indexing and selection in these arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series as dictionary\n",
    "\n",
    "Like a dictionary, the ``Series`` object provides a mapping from a collection of keys to a collection of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.Series([0.25, 0.5, 0.75, 1.0], index=['a', 'b', 'c', 'd'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use **dictionary-like Python expressions** and methods to examine the keys/indices and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Series`` objects can even be **modified with a dictionary-like syntax.**\n",
    "\n",
    "\n",
    "Just as you can extend a dictionary by assigning to a new key, you can extend a ``Series`` by assigning to a new index value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning a new item to Pandas Series  \n",
    "data['e'] = 1.25\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series as one-dimensional array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ``Series`` can provides array-style **item selection, slices, masking, and fancy indexing** via the same basic mechanisms as **NumPy arrays**\n",
    "\n",
    "\n",
    "Examples of these are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by explicit index\n",
    "data['a':'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by implicit integer index\n",
    "data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking\n",
    "data[(data > 0.3) & (data < 0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy indexing\n",
    "data[['a', 'e']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "\n",
    "Among these, slicing may be the source of the most confusion.\n",
    "Notice that when slicing with an explicit index (i.e., ``data['a':'c']``), the final index (upper band) is *included* in the slice, while when slicing with an implicit index (i.e., ``data[0:2]``), the final index (upper band) is *excluded* from the slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexers: loc, iloc\n",
    "\n",
    "\n",
    "Pandas provides some special **indexer attributes** that explicitly expose certain indexing schemes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "First, the ``loc`` attribute allows indexing and slicing that always references the **explicit index:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75, 1.0], index=['a', 'b', 'c', 'd'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``iloc`` attribute allows indexing and slicing that always references the **implicit Python-style index:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "One guiding principle of Python code is that \"explicit is better than implicit.\"\n",
    "\n",
    "\n",
    "\n",
    "The explicit nature of ``loc`` and ``iloc`` make them very useful in maintaining clean and readable code; especially in the case of integer indexes, I recommend using these both to make code easier to read and understand, and to prevent subtle bugs due to the mixed indexing/slicing convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection in DataFrame\n",
    "\n",
    "A ``DataFrame`` acts in many ways like:\n",
    "\n",
    "- A **dictionary of ``Series``**\n",
    "- A **two-dimensional array,** \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame as a dictionary\n",
    "\n",
    "The first analogy we will consider is the ``DataFrame`` as a dictionary of related ``Series`` objects.\n",
    "Let's return to our example of areas and populations of states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = pd.Series({'California': 423967, 'Texas': 695662,\n",
    "                  'New York': 141297, 'Florida': 170312,\n",
    "                  'Illinois': 149995})\n",
    "pop = pd.Series({'California': 38332521, 'Texas': 26448193,\n",
    "                 'New York': 19651127, 'Florida': 19552860,\n",
    "                 'Illinois': 12882135})\n",
    "\n",
    "# Join to Series using dic syntax to create a Dataframe\n",
    "data = pd.DataFrame({'area':area, 'pop':pop})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual ``Series`` that make up the columns of the ``DataFrame`` can be **accessed via dictionary-style indexing** of the column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic like indexing \n",
    "data['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, we can use **attribute-style access** with column names that are strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribute-style of indexing\n",
    "data.area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Like with the ``Series`` objects discussed earlier, this dictionary-style syntax can also be **used to modify** the object, in this case adding a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['density'] = data['pop'] / data['area']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a preview of the straightforward syntax of element-by-element arithmetic between ``Series`` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame as two-dimensional array\n",
    "\n",
    "As mentioned previously, we can also view the ``DataFrame`` as an enhanced **two-dimensional array.**\n",
    "\n",
    "\n",
    "We can examine the raw underlying data array using the ``values`` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many familiar **array-like** observations can be done on the ``DataFrame`` itself.\n",
    "\n",
    "\n",
    "For example, we can **transpose** the full ``DataFrame`` to swap rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and passing a single \"index\" to a ``DataFrame`` accesses a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Thus for array-style indexing, we need another convention.\n",
    "Here Pandas again uses the ``loc``, ``iloc`` indexers mentioned earlier.\n",
    "\n",
    "\n",
    "Using the ``iloc`` indexer, we can index the underlying array as if it is a simple NumPy array, but the ``DataFrame`` index and column labels are maintained in the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:3, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, using the ``loc`` indexer we can index the underlying data in an array-like style but using the **explicit index** and **column names:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:'Illinois', :'pop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any of the familiar NumPy-style data access patterns can be used within these indexers.\n",
    "For example, in the ``loc`` indexer we can combine masking and fancy indexing as in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.density > 100, ['pop', 'density']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify values using ``iloc``\n",
    "Any of these indexing conventions may also be **used to set or modify values;** this is done in the standard way that you might be accustomed to from working with NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0, 2] = 90\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build up your fluency in Pandas data manipulation, I suggest spending some time with a simple ``DataFrame`` and exploring the types of indexing, slicing, masking, and fancy indexing that are allowed by these various indexing approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional indexing conventions\n",
    "\n",
    "There are a couple extra indexing conventions that might seem at odds with the preceding discussion, but nevertheless can be very useful in practice.\n",
    "First, while *indexing* refers to columns, *slicing* refers to rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Florida':'Illinois']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such slices can also refer to rows by number rather than by index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, direct masking operations are also interpreted row-wise rather than column-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.density > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ufuncs in Pandas\n",
    "\n",
    "One of the essential pieces of NumPy is vectorized operations (ufuncs); The ability to perform quick element-wise operations, both with basic arithmetic (addition, subtraction, multiplication, etc.) and with more sophisticated operations (trigonometric functions, exponential and logarithmic functions, etc.)\n",
    "\n",
    "\n",
    "Pandas inherits much of this functionality from NumPy and vectorized operations (ufuncs).\n",
    "\n",
    "Pandas includes a couple useful twists, however: \n",
    "\n",
    "- For **unary operations** like **negation** and **trigonometric functions**, these ufuncs will **preserve index and column labels** in the output.\n",
    "\n",
    "\n",
    "- For **binary** operations such as **addition** and **multiplication,** Pandas will automatically **align indices** when passing the objects to the ufunc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ufuncs: Index Preservation (unary operations)\n",
    "\n",
    "Because Pandas is designed to work with NumPy, any NumPy ufunc will work on Pandas ``Series`` and ``DataFrame`` objects.\n",
    "Let's start by defining a simple ``Series`` and ``DataFrame`` on which to demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random Pandas Series\n",
    "rng = np.random.RandomState(42)\n",
    "ser = pd.Series(rng.randint(0, 10, 4))\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random Pandas Dataframe\n",
    "df = pd.DataFrame(rng.randint(0, 10, (3, 4)),\n",
    "                  columns=['A', 'B', 'C', 'D'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply a NumPy ufunc on either of these objects, the result will be another Pandas object *with the indices preserved:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(ser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, for a slightly more complex calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sin(df * np.pi / 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFuncs: Index Alignment (binary operations)\n",
    "\n",
    "For **binary operations** on two ``Series`` or ``DataFrame`` objects, Pandas will **align indices** in the process of performing the operation.\n",
    "This is very convenient when working with incomplete data, as we'll see in some of the examples that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index alignment in Series\n",
    "\n",
    "As an example, suppose we are **combining two different data sources,** and find only the **top three** US states by **area** and the **top three** US states by **population**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = pd.Series({'Alaska': 1723337, 'Texas': 695662,\n",
    "                  'California': 423967}, name='area')\n",
    "population = pd.Series({'California': 38332521, 'Texas': 26448193,\n",
    "                        'New York': 19651127}, name='population')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Texas** and **California** are the common rows.  \n",
    "Let's see what happens when we **divide** these to compute the **population density:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population / area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting array contains the ***union* of indices** of the two input arrays, which could be determined using standard Python set arithmetic on these indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area.index | population.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any item for which one or the other does not have an entry is marked with ``NaN``, or \"Not a Number,\" which is how Pandas marks missing data.\n",
    "\n",
    "\n",
    "Any missing values are filled in with ``NaN`` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series([2, 4, 6], index=[0, 1, 2])\n",
    "B = pd.Series([1, 3, 5], index=[1, 2, 3])\n",
    "A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using ``NaN`` values is not the desired behavior, the **fill value** can be modified using appropriate **object methods** in place of the **operators.**\n",
    "\n",
    "- What is **object methods**?\n",
    "\n",
    "\n",
    "For example, calling ``A.add(B)`` is equivalent to calling ``A + B``, but allows optional explicit specification of the fill value for any elements in ``A`` or ``B`` that might be missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.add(B, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index alignment in DataFrame\n",
    "\n",
    "A similar type of alignment takes place for *both* **columns** and **indices** when performing operations on ``DataFrame``s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.DataFrame(rng.randint(0, 20, (2, 2)),\n",
    "                 columns=list('AB'))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder \n",
    "list('ABC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = pd.DataFrame(rng.randint(0, 10, (3, 3)),\n",
    "                 columns=list('BAC'))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that indices are **aligned correctly irrespective of their order** in the two objects, and **indices in the result are sorted.**\n",
    "\n",
    "\n",
    "As was the case with ``Series``, we can use the associated **object's arithmetic method** and pass any desired ``fill_value`` to be used in place of missing entries.\n",
    "\n",
    "\n",
    "Here we'll fill with the mean of all values in ``A`` (computed by first stacking the rows of ``A``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = A.stack().mean()\n",
    "A.add(B, fill_value=fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table lists Python operators and their equivalent Pandas object methods:\n",
    "\n",
    "| Python Operator | Pandas Method(s)                      |\n",
    "|-----------------|---------------------------------------|\n",
    "| ``+``           | ``add()``                             |\n",
    "| ``-``           | ``sub()``, ``subtract()``             |\n",
    "| ``*``           | ``mul()``, ``multiply()``             |\n",
    "| ``/``           | ``truediv()``, ``div()``, ``divide()``|\n",
    "| ``//``          | ``floordiv()``                        |\n",
    "| ``%``           | ``mod()``                             |\n",
    "| ``**``          | ``pow()``                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between **data found in many tutorials** and data in the **real world** is that real-world data is **rarely clean** and **homogeneous.**\n",
    "\n",
    "\n",
    "In particular, many interesting datasets will have some amount of **data missing.**\n",
    "\n",
    "\n",
    "To make matters even more complicated, different data sources may indicate missing data in different ways.\n",
    "\n",
    "\n",
    "In this section, we will discuss some general considerations for missing data, discuss **how Pandas chooses to represent it,** and demonstrate some built-in Pandas tools for **handling missing data** in Python.\n",
    "\n",
    "\n",
    "Here and throughout this module, we'll refer to missing data in general as *null*, *NaN*, or *NA* values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade-Offs in Missing Data Conventions\n",
    "\n",
    "There are a number of **schemes** that have been developed to indicate the **presence of missing data** in a table or DataFrame.\n",
    "\n",
    "\n",
    "Generally, they revolve around one of **two strategies:**\n",
    "\n",
    "\n",
    "\n",
    " - **Masking approach:** In the masking approach, the **mask** might be an **entirely separate Boolean array,** or it may involve appropriation of **one bit** in the data representation to **locally indicate** the null status of a value.\n",
    "\n",
    "- **Sentinel approach:** In the **sentinel approach**, the sentinel value could be some data-specific convention, such as indicating a missing integer value with -9999 or some rare bit pattern, or it could be a more global convention, such as indicating a missing floating-point value with NaN (Not a Number).\n",
    "\n",
    "\n",
    "### Trade-offs:\n",
    "None of these approaches is without **trade-offs:** use of a **separate mask array** requires **allocation of an additional Boolean array,** which adds **overhead** in both storage and computation.\n",
    "\n",
    "\n",
    "A **sentinel value** reduces the range of valid values that can be represented, and may require extra (often non-optimized) logic in CPU and GPU arithmetic. Common special values like **NaN** are not available for all data types.\n",
    "\n",
    "\n",
    "Different languages and systems use different conventions.\n",
    "For example, **the R** language uses reserved bit patterns within each data type as sentinel values indicating missing data, while the SciDB system uses an extra byte attached to every cell which indicates a NA state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data in Pandas\n",
    "\n",
    "The way in which **Pandas handles missing values** is constrained by its reliance on the **NumPy package,** which **does not have a built-in notion of NA values** for non-floating-point data types.\n",
    "\n",
    "Pandas could have followed **R's** lead in specifying **bit patterns** for each individual data type to **indicate nullness,** but this approach turns out to be rather unwieldy.\n",
    "\n",
    "### Why not?\n",
    "\n",
    "While **R** contains **four basic data types,** NumPy supports ***far* more** than this: \n",
    "\n",
    "for example, while R has a single integer type, NumPy supports *fourteen* basic integer types once you account for available precisions, signedness, and endianness of the encoding.\n",
    "\n",
    "\n",
    "**Reserving** a specific **bit pattern** in **all available NumPy types** would lead to an unwieldy amount of **overhead** in special-casing various operations for various types, likely even requiring a new fork of the NumPy package. \n",
    "\n",
    "\n",
    "Further, for the smaller data types (such as 8-bit integers), **sacrificing a bit** to use **as a mask** will significantly **reduce the range** of values it can represent.\n",
    "\n",
    "\n",
    "\n",
    "With these constraints in mind, **Pandas chose to use sentinels** for **missing data,** and further chose to use two already-existing Python null values:\n",
    "\n",
    "the special floating-point **``NaN``** value, and the Python **``None``** object.\n",
    "\n",
    "\n",
    "This choice has some side effects, as we will see, but in practice ends up being a good compromise in most cases of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``None``: Pythonic missing data\n",
    "\n",
    "The first sentinel value used by Pandas is ``None``, a Python singleton object (single design pattern) that is often used for missing data in Python code.\n",
    "\n",
    "\n",
    "Because it is a **Python object,** ``None`` cannot be used in any arbitrary NumPy/Pandas array, but only in arrays with data type ``'object'`` (i.e., **arrays of Python objects**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = np.array([1, None, 3, 4])\n",
    "vals1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ``dtype=object`` means that the best common type representation NumPy could infer for the contents of the array is that they are **Python objects.**\n",
    "\n",
    "\n",
    "While this kind of **object array** is useful for some purposes **BUT** any operations on the data will be done at the **Python level,** with much **more overhead** than the typically fast operations seen for arrays with native types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in ['object', 'int']:\n",
    "    print(\"dtype =\", dtype)\n",
    "    %timeit np.arange(1E6, dtype=dtype).sum()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **use** of **Python objects** in an array also means that if you perform **aggregations** like ``sum()`` or ``min()`` across an array with a ``None`` value, you will generally **get an error:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reflects the fact that addition between an integer and ``None`` is undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``NaN``: Missing numerical data\n",
    "\n",
    "The other missing data representation, ``NaN`` (acronym for ***Not a Number***), is different;\n",
    "\n",
    "It is a **special floating-point value** recognized by all systems that use the standard IEEE floating-point representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2 = np.array([1, np.nan, 3, 4]) \n",
    "vals2.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that NumPy chose a **native floating-point** type for this array: this means that unlike the object array from before, **this array supports fast operations** pushed into compiled code.\n",
    "\n",
    "\n",
    "**NOTE:** You should be aware that ``NaN`` is a bit like a data virusit infects any other object it touches.\n",
    "\n",
    "Regardless of the operation, **the result of arithmetic with ``NaN`` will be another ``NaN``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 *  np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this means that **aggregates over the values**  don't result in an **error** (like ``None``)but not always **useful.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NumPy** does provide some **special aggregations** that will ignore these missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nansum(vals2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmin(vals2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.nanmax(vals2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that ``NaN`` is specifically a **floating-point value;** there is no equivalent NaN value for integers, strings, or other types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN and None in Pandas\n",
    "\n",
    "``NaN`` and ``None`` both have their place, and **Pandas** is built to handle the two of them **nearly interchangeably,** converting between them where appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1, np.nan, 2, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For types that don't have an available sentinel value, Pandas **automatically type-casts** when NA values are present.\n",
    "\n",
    "\n",
    "For example, if we set a value in an **integer array** to ``np.nan``, it will automatically be upcast to a **floating-point type** to accommodate the NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(range(2), dtype=int)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically be upcast to a **floating-point type** to accommodate the NaN\n",
    "x[0] = None\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in addition to casting the **integer array** to **floating point,** Pandas automatically converts the ``None`` to a ``NaN`` value.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The following table lists the **upcasting conventions in Pandas** when NA values are introduced:\n",
    "\n",
    "|Typeclass     | Conversion When Storing NAs | NA Sentinel Value      |\n",
    "|--------------|-----------------------------|------------------------|\n",
    "| ``floating`` | No change                   | ``np.nan``             |\n",
    "| ``object``   | No change                   | ``None`` or ``np.nan`` |\n",
    "| ``integer``  | Cast to ``float64``         | ``np.nan``             |\n",
    "| ``boolean``  | Cast to ``object``          | ``None`` or ``np.nan`` |\n",
    "\n",
    "Keep in mind that in **Pandas, string data** is always stored with an ``object`` dtype (Yeah.. it is slow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operating on Null Values\n",
    "\n",
    "As we have seen, Pandas treats ``None`` and ``NaN`` as essentially **interchangeable** for indicating **missing or null** values.\n",
    "\n",
    "\n",
    "To facilitate this convention, there are several useful methods for **detecting, removing,** and **replacing** null values in Pandas data structures.\n",
    "They are:\n",
    "\n",
    "- ``isnull()``: Generate a **boolean mask** indicating missing values\n",
    "- ``notnull()``: Opposite of ``isnull()``\n",
    "- ``dropna()``: Return a filtered version of the data\n",
    "- ``fillna()``: Return a copy of the data with missing values filled or imputed\n",
    "\n",
    "We will conclude this section with a brief exploration and demonstration of these routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting null values using ``isnull()`` and ``notnull()`` \n",
    "Pandas data structures have two useful methods for detecting null data: ``isnull()`` and ``notnull()``.\n",
    "Either one will return a Boolean mask over the data. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 'hello', None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Boolean masks can be used directly as a ``Series`` or ``DataFrame`` index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``isnull()`` and ``notnull()`` methods produce similar Boolean results for ``DataFrame``s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping null values using ``dropna()`` and ``fillna()``\n",
    "\n",
    "In addition to the masking used before, there are the convenience methods, ``dropna()``\n",
    "(which removes NA values) and ``fillna()`` (which fills in NA values). \n",
    "\n",
    "For a ``Series``,the result is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a ``DataFrame``, there are more options.\n",
    "Consider the following ``DataFrame``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1,      np.nan, 2],\n",
    "                   [2,      3,      5],\n",
    "                   [np.nan, 4,      6]])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot drop single values from a ``DataFrame``; we can **ONLY** drop **full rows** or **full columns.**\n",
    "\n",
    "\n",
    "\n",
    "By **default**, ``dropna()`` will **drop all rows** in which *any* null value is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can drop NA values along a different axis; ``axis=1`` **drops all columns** containing a null value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this drops some good data as well; you might rather be interested in dropping rows or columns with *all* NA values, or a majority of NA values.\n",
    "This can be specified through the ``how`` or ``thresh`` parameters, which allow fine control of the number of nulls to allow through.\n",
    "\n",
    "The default is ``how='any'``, such that any row or column (depending on the ``axis`` keyword) containing a null value will be dropped.\n",
    "You can also specify ``how='all'``, which will only drop rows/columns that are *all* null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='columns', how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finer-grained control, the ``thresh`` parameter lets you specify a minimum number of non-null values for the row/column to be kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='rows', thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the first and last row have been dropped, because they contain only two non-null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling null values\n",
    "\n",
    "Sometimes rather than dropping NA values, you'd rather replace them with a valid value.\n",
    "This value might be a single number like zero, or it might be some sort of imputation or interpolation from the good values.\n",
    "You could do this in-place using the ``isnull()`` method as a mask, but because it is such a common operation Pandas provides the ``fillna()`` method, which returns a copy of the array with the null values replaced.\n",
    "\n",
    "Consider the following ``Series``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill NA entries with a single value, such as zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a forward-fill to propagate the previous value forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward-fill\n",
    "data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can specify a back-fill to propagate the next values backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back-fill\n",
    "data.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For ``DataFrame``s, the options are similar, but we can also specify an ``axis`` along which the fills take place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if a previous value is not available during a forward fill, the NA value remains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Luck!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
